"""
SEO åˆ†æèˆ‡å„ªåŒ–æœå‹™
8 é …è©•åˆ†å¼•æ“ï¼Œé‡å° Dcard å¹³å° SEO ç‰¹æ€§è¨­è¨ˆ
"""
import re
import logging
from typing import Optional

from google import genai
from google.genai import types

from app.config import settings
from app.services.gemini_utils import strip_markdown, track_gemini_usage, track_anthropic_usage, is_anthropic_model

logger = logging.getLogger(__name__)


SEO_OPTIMIZE_PROMPT = """ä½ æ˜¯ä¸€ä½ Dcard SEO å„ªåŒ–å°ˆå®¶ï¼Œå°ˆé–€å„ªåŒ–æ–‡ç« ä½¿å…¶ç™»ä¸Š Google æœå°‹é¦–é ã€‚ä»¥ä¸‹æ˜¯åŸºæ–¼å¯¦éš›ä¸Šæ¦œæ–‡ç« é€†å‘åˆ†æçš„å„ªåŒ–ç­–ç•¥ã€‚

===== æ¨™é¡Œå„ªåŒ–ï¼ˆæœ€é«˜å„ªå…ˆï¼‰ =====
æ¨™é¡Œæ±ºå®š 60% çš„ Google æ’åã€‚æœ€ä½³æ ¼å¼ï¼š
ã€{å¹´ä»½}{é—œéµå­—}æ¨è–¦ã€‘{ç—›é»Hook}ï¼{N}æ¬¾Dcard/PTTç†±è­°è©•æ¯”ï¼š{å—çœ¾æ¨™ç±¤}ã€{å“ç‰Œå}
- æ¨™é¡Œ 20-35 å­—ï¼ˆGoogle SERP æœ€ä½³é¡¯ç¤ºç¯„åœï¼‰
- å¿…é ˆåŒ…å«ï¼šå¹´ä»½ã€ä¸»é—œéµå­—ã€ã€Œæ¨è–¦ã€ã€ã€ŒDcard/PTTã€
- åŠ å…¥å“ç‰Œåï¼ˆé•·å°¾é—œéµå­—å…¥å£ï¼‰
- åŠ å…¥å—çœ¾æ¨™ç±¤ï¼ˆå°è³‡/ç§Ÿå±‹/å­¸ç”Ÿç­‰ï¼Œæ•ç²æ„åœ–æœå°‹ï¼‰

===== é—œéµå­—ç­–ç•¥ =====
1. é¦–æ®µ 100 å­—å…§è‡ªç„¶å¸¶å…¥ä¸»è¦é—œéµå­—ï¼ˆGoogle åˆ¤æ–·æ–‡ç« ä¸»é¡Œçš„ä¾æ“šï¼‰
2. é—œéµå­—å¯†åº¦æ§åˆ¶åœ¨ 1-2%ï¼ˆä¸è¦å †ç Œï¼‰
3. é—œéµå­—å‡å‹»åˆ†ä½ˆåœ¨æ–‡ç« å››ç­‰åˆ†å€é–“
4. åŠ å…¥èªç¾©ç›¸é—œè©ï¼ˆåŒç¾©è©ã€ç›¸é—œæ¦‚å¿µã€ä½¿ç”¨å ´æ™¯è©ï¼‰ï¼Œä¸åªé‡è¤‡åŒä¸€é—œéµå­—
5. ç”¢å“å“ç‰Œåå’Œå‹è™Ÿè¦å®Œæ•´å¯«å‡ºï¼ˆé•·å°¾é—œéµå­—å…¥å£ï¼‰

===== çµæ§‹å„ªåŒ– =====
ç¢ºä¿æ–‡ç« åŒ…å«ä»¥ä¸‹ Google æ’ååŠ åˆ†çµæ§‹ï¼ˆè‹¥ç¼ºå°‘è«‹è£œä¸Šï¼‰ï¼š
- â­ 10ç§’éœ€æ±‚å°ç…§æ‡¶äººåŒ…ï¼ˆå¿«é€Ÿå°èˆªï¼Œé™ä½è·³å‡ºç‡ï¼‰
- ğŸ” é¸è³¼é‡é»ï¼ˆ3 å€‹é‡é»ï¼Œæ¯å€‹æœ‰å°æ¨™é¡Œ + è©³è§£ï¼‰
- æ¯æ¬¾ç”¢å“çµ±ä¸€æ ¼å¼ï¼ˆğŸ§ä¸Šæ¦œç†ç”±/ğŸ’¬å¿ƒå¾—/ğŸ“Œç‰¹è‰²/ğŸ’¡æé†’/ğŸ’°åƒ¹æ ¼/ğŸ“‹è¦æ ¼ï¼‰
- ğŸ¯ ç³¾çµçµ‚çµï¼ˆæŒ‰éœ€æ±‚å ´æ™¯æœ€çµ‚æ¨è–¦ï¼Œæå‡åœç•™æ™‚é–“ï¼‰
- â“ FAQ å€å¡Šï¼ˆè‡³å°‘ 5 é¡Œï¼Œä½¿ç”¨ã€ŒQï¼š/Aï¼šã€çµæ§‹åŒ–æ ¼å¼ï¼Œå•é¡Œå¿…é ˆæ˜¯ Google å¸¸æœå•é¡Œï¼‰

===== å…§å®¹å„ªåŒ– =====
- æ–‡ç« ç¸½å­—æ•¸è‡³å°‘ 3000 å­—ï¼ˆGoogle æ·±åº¦å…§å®¹åˆ¤å®šé–€æª»ï¼‰
- å£èªåŒ–ã€å¹´è¼•åŒ–çš„ Dcard é¢¨æ ¼ï¼ˆã€Œè€å¯¦èªªã€ã€ŒçœŸçš„ã€ã€Œè¶…ã€ç­‰ï¼‰
- å¿ƒå¾—éƒ¨åˆ†è¦æœ‰å…·é«”å ´æ™¯æè¿°ï¼ˆæº«åº¦ã€æ™‚é–“ã€åœ°é»ã€è§¸æ„Ÿï¼‰
- æ¯æ¬¾ç”¢å“ä¹‹é–“ç”¨åˆ†éš”ç·š(===)éš”é–‹

âš ï¸ æ ¼å¼è¦å‰‡ï¼š
- ä¸è¦ä½¿ç”¨ä»»ä½• Markdown èªæ³•ï¼ˆä¸è¦ç”¨ **ç²—é«”**ã€### æ¨™é¡Œã€- åˆ—è¡¨ç­‰ï¼‰
- ç›´æ¥ç”¨ç´”æ–‡å­—ï¼Œæ­é…è¡¨æƒ…ç¬¦è™Ÿå’Œåˆ†éš”ç·š(===)æ’ç‰ˆ
- ä¿ç•™åŸæ–‡ä¸­çš„åœ–ç‰‡æ¨™è¨˜ {{IMAGE:...}} ä¸è¦å‹•

âš ï¸ ç¦æ­¢åœ¨æ–‡ç« ä¸­å‡ºç¾ä»»ä½• SEO ç­–ç•¥èªªæ˜æ–‡å­—ï¼ˆä¾‹å¦‚ã€Œæ¶ä½” Google é¦–é ã€ã€Œæå‡æœå°‹æ’åã€ã€Œé•·å°¾é—œéµå­—ã€ç­‰ï¼‰ï¼Œè®€è€…ä¸éœ€è¦çŸ¥é“é€™äº›ã€‚

è«‹ç›´æ¥è¼¸å‡ºå„ªåŒ–å¾Œçš„å®Œæ•´æ–‡ç« ï¼ˆå«å„ªåŒ–å¾Œçš„æ¨™é¡Œï¼‰ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡‹æˆ–å‰è¨€ã€‚"""


# æ“´å¤§çš„åœç”¨è©è¡¨
STOP_WORDS = {
    'å¦‚æœä½ ', 'ç‚ºä»€éº¼', 'æ€éº¼æ¨£', 'å“ªå€‹å¥½', 'é€™å€‹', 'é‚£å€‹', 'ä»€éº¼',
    'å¯ä»¥', 'å°±æ˜¯', 'ä¸æ˜¯', 'å·²ç¶“', 'æ‰€ä»¥', 'å› ç‚º', 'ä½†æ˜¯', 'è€Œä¸”',
    'é‚„æ˜¯', 'æˆ–è€…', 'å¦‚æœ', 'é›–ç„¶', 'åªè¦', 'åªæœ‰', 'ä¸€å®š', 'æ‡‰è©²',
    'çœŸçš„', 'å…¶å¯¦', 'å¤§å®¶', 'æˆ‘å€‘', 'ä»–å€‘', 'è‡ªå·±', 'è¦ºå¾—', 'çŸ¥é“',
    'ç¾åœ¨', 'ä»Šå¤©', 'æœ€è¿‘', 'ä¹‹å‰', 'ä¹‹å¾Œ', 'æ™‚å€™', 'åœ°æ–¹', 'æ±è¥¿',
    'å•é¡Œ', 'æ–¹æ³•', 'éƒ¨åˆ†', 'ä¸€äº›', 'é€™äº›', 'é‚£äº›', 'å„ä½', 'åŒå­¸',
    'çœ‹çœ‹', 'ä¾†èªª', 'ä¸€ä¸‹', 'æ€éº¼', 'åˆ†äº«', 'é—œæ–¼',
}


class SeoService:
    """SEO åˆ†ææœå‹™ â€” 8 é …è©•åˆ†å¼•æ“"""

    # è©•åˆ†æ¬Šé‡ï¼ˆç¸½å’Œ = 100ï¼‰
    WEIGHTS = {
        "title_seo": 15,          # æ¨™é¡Œ SEOï¼ˆé•·åº¦ 20-35 å­— + å«é—œéµå­—ï¼‰
        "keyword_density": 20,    # é—œéµå­—å¯†åº¦ï¼ˆ1-2% æ»¿åˆ†ï¼‰
        "keyword_placement": 15,  # é—œéµå­—åˆ†ä½ˆï¼ˆé¦–æ®µ + å‡å‹»åº¦ï¼‰
        "content_structure": 15,  # å…§å®¹çµæ§‹ï¼ˆæ®µè½ + åˆ—è¡¨ + åˆ†éš”ç·šï¼‰
        "content_length": 15,     # å…§å®¹é•·åº¦ï¼ˆ1500-2500 å­—ï¼‰
        "faq_quality": 10,        # FAQ çµæ§‹
        "media_usage": 5,         # åœ–ç‰‡ä½¿ç”¨
        "readability": 5,         # å¯è®€æ€§
    }

    def __init__(self):
        self._gemini_client = None
        self._anthropic_client = None

    @property
    def gemini_client(self):
        if self._gemini_client is None:
            if not settings.GOOGLE_API_KEY:
                raise ValueError("GOOGLE_API_KEY æœªè¨­å®šï¼Œè«‹åœ¨ .env ä¸­è¨­å®š")
            self._gemini_client = genai.Client(api_key=settings.GOOGLE_API_KEY)
        return self._gemini_client

    @property
    def anthropic_client(self):
        if self._anthropic_client is None:
            if not settings.ANTHROPIC_API_KEY:
                raise ValueError("ANTHROPIC_API_KEY æœªè¨­å®šï¼Œè«‹åœ¨ .env ä¸­è¨­å®š")
            import anthropic
            self._anthropic_client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        return self._anthropic_client

    @staticmethod
    def _extract_keywords_from_title(title: str) -> list:
        """å¾æ¨™é¡Œè‡ªå‹•æå–é—œéµå­—ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
        keywords = []

        # 1. æå–ã€ã€‘å…§å®¹ï¼ˆæ¸…é™¤å¹´ä»½ï¼‰
        bracket_words = re.findall(r'ã€(.+?)ã€‘', title)
        for w in bracket_words:
            cleaned = re.sub(r'\d{4}', '', w).strip()
            if cleaned:
                # æŒ‰æ¨™é»/ç¬¦è™Ÿæ‹†åˆ†
                parts = re.split(r'[ï¼Œ,ã€/\|ï½œ\s]+', cleaned)
                for part in parts:
                    part = part.strip()
                    if 2 <= len(part) <= 8 and part not in STOP_WORDS:
                        keywords.append(part)

        # 2. æ¨™é¡Œä¸»é«”ï¼ˆå»é™¤ã€ã€‘çš„éƒ¨åˆ†ï¼‰ï¼ŒæŒ‰æ¨™é»/ç¬¦è™Ÿæ‹†åˆ†
        title_body = re.sub(r'ã€.+?ã€‘', ' ', title)
        # æŒ‰æ¨™é»å’Œç¬¦è™Ÿæ‹†åˆ†
        segments = re.split(r'[ï¼Œ,ã€‚ï¼ï¼Ÿ!?\sã€/\|ï½œï½~ï¼š:ï¼ˆï¼‰()\-]+', title_body)
        for seg in segments:
            seg = seg.strip()
            if not seg:
                continue
            # æå– 2-8 å­—ä¸­æ–‡ç‰‡æ®µ
            cn_matches = re.findall(r'[\u4e00-\u9fff]{2,8}', seg)
            for w in cn_matches:
                if w not in keywords and w not in STOP_WORDS:
                    keywords.append(w)
                    # é•·è©æ¼¸é€²æ‹†åˆ†ï¼ˆ>4 å­—æ‹†æˆå­è©ï¼‰
                    if len(w) > 4:
                        for start in range(len(w) - 1):
                            for end in range(start + 2, min(start + 5, len(w) + 1)):
                                sub = w[start:end]
                                if 2 <= len(sub) <= 4 and sub not in keywords and sub not in STOP_WORDS:
                                    keywords.append(sub)

        return keywords[:10]

    def analyze(self, title: str, content: str, keywords: Optional[list] = None) -> dict:
        """åˆ†ææ–‡ç«  SEO åˆ†æ•¸ï¼ˆ8 é …æŒ‡æ¨™ï¼‰"""
        breakdown = {}
        suggestions = []

        if not keywords:
            keywords = self._extract_keywords_from_title(title)

        # ===== 1. æ¨™é¡Œ SEO (15 åˆ†) =====
        title_score = self._score_title_seo(title, keywords, suggestions)
        breakdown["title_seo"] = {
            "score": title_score,
            "max": self.WEIGHTS["title_seo"],
            "label": "æ¨™é¡Œ SEO",
        }

        # ===== 2. é—œéµå­—å¯†åº¦ (20 åˆ†) =====
        density_score, density_val = self._score_keyword_density(content, keywords, suggestions)
        breakdown["keyword_density"] = {
            "score": density_score,
            "max": self.WEIGHTS["keyword_density"],
            "label": "é—œéµå­—å¯†åº¦",
        }

        # ===== 3. é—œéµå­—åˆ†ä½ˆ (15 åˆ†) =====
        placement_score = self._score_keyword_placement(content, keywords, suggestions)
        breakdown["keyword_placement"] = {
            "score": placement_score,
            "max": self.WEIGHTS["keyword_placement"],
            "label": "é—œéµå­—åˆ†ä½ˆ",
        }

        # ===== 4. å…§å®¹çµæ§‹ (15 åˆ†) =====
        structure_score = self._score_content_structure(content, suggestions)
        breakdown["content_structure"] = {
            "score": structure_score,
            "max": self.WEIGHTS["content_structure"],
            "label": "å…§å®¹çµæ§‹",
        }

        # ===== 5. å…§å®¹é•·åº¦ (15 åˆ†) =====
        length_score = self._score_content_length(content, suggestions)
        breakdown["content_length"] = {
            "score": length_score,
            "max": self.WEIGHTS["content_length"],
            "label": "å…§å®¹é•·åº¦",
        }

        # ===== 6. FAQ çµæ§‹ (10 åˆ†) =====
        faq_score = self._score_faq_quality(content, suggestions)
        breakdown["faq_quality"] = {
            "score": faq_score,
            "max": self.WEIGHTS["faq_quality"],
            "label": "FAQ çµæ§‹",
        }

        # ===== 7. åœ–ç‰‡ä½¿ç”¨ (5 åˆ†) =====
        media_score, image_count = self._score_media_usage(content, suggestions)
        breakdown["media_usage"] = {
            "score": media_score,
            "max": self.WEIGHTS["media_usage"],
            "label": "åœ–ç‰‡ä½¿ç”¨",
        }

        # ===== 8. å¯è®€æ€§ (5 åˆ†) =====
        read_score = self._score_readability(content, suggestions)
        breakdown["readability"] = {
            "score": read_score,
            "max": self.WEIGHTS["readability"],
            "label": "å¯è®€æ€§",
        }

        total_score = sum(item["score"] for item in breakdown.values())

        # æŒ‰åš´é‡åº¦æ’åºå»ºè­°ï¼ˆåˆ†æ•¸ç™¾åˆ†æ¯”ä½çš„æ’å‰é¢ï¼‰
        suggestions.sort(key=lambda s: s.get("priority", 99) if isinstance(s, dict) else 99)
        # è½‰æˆç´”æ–‡å­—å»ºè­°åˆ—è¡¨
        suggestion_texts = []
        for s in suggestions:
            if isinstance(s, dict):
                suggestion_texts.append(s["text"])
            else:
                suggestion_texts.append(s)

        # çµ±è¨ˆè³‡è¨Š
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
        stats = {
            "title_length": len(title),
            "content_length": len(content),
            "paragraph_count": len(paragraphs),
            "image_count": image_count,
            "keyword_density": round(density_val, 2),
        }

        return {
            "score": round(total_score, 1),
            "max_score": 100,
            "grade": self._get_grade(total_score),
            "breakdown": breakdown,
            "suggestions": suggestion_texts,
            "keywords": keywords,
            "stats": stats,
        }

    # â”€â”€ å„é …è©•åˆ†å­å‡½æ•¸ â”€â”€

    def _score_title_seo(self, title: str, keywords: list, suggestions: list) -> float:
        """æ¨™é¡Œ SEO è©•åˆ†ï¼šé•·åº¦ 20-35 å­— + å«é—œéµå­—"""
        max_score = self.WEIGHTS["title_seo"]
        score = 0.0
        title_len = len(title)

        # é•·åº¦åˆ†ï¼ˆä½” 60%ï¼‰
        if 20 <= title_len <= 35:
            score += max_score * 0.6
        elif 15 <= title_len <= 45:
            score += max_score * 0.4
            suggestions.append({
                "text": f"æ¨™é¡Œ {title_len} å­—ï¼ŒGoogle SERP æœ€ä½³é¡¯ç¤ºç‚º 20-35 ä¸­æ–‡å­—",
                "priority": 2,
            })
        else:
            score += max_score * 0.15
            suggestions.append({
                "text": f"æ¨™é¡Œ {title_len} å­—ï¼Œå»ºè­°èª¿æ•´åˆ° 20-35 å­—ï¼ˆGoogle æœå°‹çµæœæœ€ä½³é¡¯ç¤ºç¯„åœï¼‰",
                "priority": 1,
            })

        # é—œéµå­—åˆ†ï¼ˆä½” 40%ï¼‰
        if keywords:
            has_keyword = any(kw in title for kw in keywords[:3])
            if has_keyword:
                score += max_score * 0.4
            else:
                score += max_score * 0.1
                suggestions.append({
                    "text": "æ¨™é¡Œä¸­ç¼ºå°‘ä¸»è¦é—œéµå­—ï¼Œå»ºè­°åœ¨æ¨™é¡Œä¸­åŒ…å«æ ¸å¿ƒé—œéµå­—",
                    "priority": 1,
                })
        else:
            score += max_score * 0.2

        return round(score, 1)

    def _score_keyword_density(self, content: str, keywords: list, suggestions: list) -> tuple:
        """é—œéµå­—å¯†åº¦è©•åˆ†ï¼šä¿®æ­£å…¬å¼ï¼Œ1-2% æ»¿åˆ†"""
        max_score = self.WEIGHTS["keyword_density"]
        total_chars = len(content)

        if not keywords or total_chars == 0:
            suggestions.append({
                "text": "ç„¡æ³•åˆ†æé—œéµå­—å¯†åº¦ï¼Œå»ºè­°åœ¨æ¨™é¡ŒåŠ å…¥ã€é—œéµå­—ã€‘æ¨™è¨˜",
                "priority": 2,
            })
            return max_score * 0.3, 0.0

        # æ­£ç¢ºå…¬å¼ï¼šæ¯å€‹é—œéµå­—çš„å‡ºç¾æ¬¡æ•¸ * è©²é—œéµå­—é•·åº¦ï¼ŒåŠ ç¸½å¾Œé™¤ä»¥ç¸½å­—æ•¸
        keyword_chars = sum(content.count(kw) * len(kw) for kw in keywords)
        density = (keyword_chars / total_chars) * 100

        if 1.0 <= density <= 2.0:
            score = max_score
        elif 0.5 <= density < 1.0 or 2.0 < density <= 3.0:
            score = max_score * 0.7
            if density < 1.0:
                suggestions.append({
                    "text": f"é—œéµå­—å¯†åº¦ {density:.1f}%ï¼Œåä½ï¼Œå»ºè­°æé«˜åˆ° 1-2%",
                    "priority": 2,
                })
            else:
                suggestions.append({
                    "text": f"é—œéµå­—å¯†åº¦ {density:.1f}%ï¼Œç•¥é«˜ï¼Œå»ºè­°æ§åˆ¶åœ¨ 1-2%",
                    "priority": 3,
                })
        elif density > 3.0:
            score = max_score * 0.3
            suggestions.append({
                "text": f"é—œéµå­—å¯†åº¦ {density:.1f}%ï¼Œéé«˜æœ‰å †ç Œé¢¨éšªï¼Œå»ºè­°é™åˆ° 1-2%",
                "priority": 1,
            })
        else:
            score = max_score * 0.2
            suggestions.append({
                "text": f"é—œéµå­—å¯†åº¦ {density:.1f}%ï¼Œéä½ï¼Œå»ºè­°è‡ªç„¶èå…¥æ›´å¤šé—œéµå­—åˆ° 1-2%",
                "priority": 1,
            })

        return round(score, 1), density

    def _score_keyword_placement(self, content: str, keywords: list, suggestions: list) -> float:
        """é—œéµå­—åˆ†ä½ˆè©•åˆ†ï¼šé¦–æ®µ 100 å­—å«é—œéµå­— + åˆ†ä½ˆå‡å‹»åº¦"""
        max_score = self.WEIGHTS["keyword_placement"]

        if not keywords:
            return max_score * 0.3

        score = 0.0

        # é¦–æ®µ 100 å­—åŒ…å«é—œéµå­—ï¼ˆä½” 50%ï¼‰
        first_100 = content[:100]
        has_early_keyword = any(kw in first_100 for kw in keywords[:3])
        if has_early_keyword:
            score += max_score * 0.5
        else:
            suggestions.append({
                "text": "é¦–æ®µ 100 å­—æœªåŒ…å«é—œéµå­—ï¼Œå° Google æ’åå½±éŸ¿æ¥µå¤§",
                "priority": 1,
            })

        # åˆ†ä½ˆå‡å‹»åº¦ï¼ˆä½” 50%ï¼‰ï¼šå°‡å…§å®¹åˆ†æˆ 4 ç­‰åˆ†ï¼Œçœ‹æ¯æ®µæ˜¯å¦éƒ½æœ‰é—œéµå­—
        if len(content) >= 200:
            quarter = len(content) // 4
            quarters_with_kw = 0
            for i in range(4):
                start = i * quarter
                end = (i + 1) * quarter if i < 3 else len(content)
                segment = content[start:end]
                if any(kw in segment for kw in keywords[:5]):
                    quarters_with_kw += 1

            spread_ratio = quarters_with_kw / 4
            score += max_score * 0.5 * spread_ratio
            if quarters_with_kw < 3:
                suggestions.append({
                    "text": f"é—œéµå­—åƒ…åˆ†ä½ˆåœ¨ {quarters_with_kw}/4 æ®µè½å€é–“ï¼Œå»ºè­°æ›´å‡å‹»åˆ†ä½ˆ",
                    "priority": 3,
                })
        else:
            score += max_score * 0.25

        return round(score, 1)

    def _score_content_structure(self, content: str, suggestions: list) -> float:
        """å…§å®¹çµæ§‹è©•åˆ†ï¼šæ®µè½æ•¸é‡ + é•·åº¦æ§åˆ¶ + åˆ—è¡¨ä½¿ç”¨ + åˆ†éš”ç·š"""
        max_score = self.WEIGHTS["content_structure"]
        score = 0.0

        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
        para_count = len(paragraphs)

        # æ®µè½æ•¸é‡ï¼ˆä½” 35%ï¼‰
        if 8 <= para_count <= 25:
            score += max_score * 0.35
        elif 5 <= para_count <= 30:
            score += max_score * 0.2
            suggestions.append({
                "text": f"æ®µè½æ•¸ {para_count}ï¼Œå»ºè­° 8-25 æ®µæå‡é–±è®€é«”é©—",
                "priority": 3,
            })
        else:
            score += max_score * 0.1
            suggestions.append({
                "text": f"æ®µè½æ•¸ {para_count}ï¼Œ{'éå°‘' if para_count < 5 else 'éå¤š'}ï¼Œå»ºè­° 8-25 æ®µ",
                "priority": 2,
            })

        # æ®µè½é•·åº¦æ§åˆ¶ï¼ˆä½” 25%ï¼‰ï¼šç„¡è¶…é•·æ®µè½
        if paragraphs:
            avg_len = sum(len(p) for p in paragraphs) / len(paragraphs)
            long_paras = sum(1 for p in paragraphs if len(p) > 300)
            if long_paras == 0 and 30 <= avg_len <= 200:
                score += max_score * 0.25
            elif long_paras <= 2:
                score += max_score * 0.15
            else:
                score += max_score * 0.05
                suggestions.append({
                    "text": f"æœ‰ {long_paras} å€‹æ®µè½è¶…é 300 å­—ï¼Œå»ºè­°æ‹†åˆ†",
                    "priority": 3,
                })

        # åˆ—è¡¨/é …ç›®ç¬¦è™Ÿä½¿ç”¨ï¼ˆä½” 20%ï¼‰
        list_items = len(re.findall(r'^[-*â—ğŸ‘‰â¡ï¸âœ…âœ¨â¤ï¸ğŸ˜ğŸ’°ğŸ”â“]\s*', content, re.MULTILINE))
        if list_items >= 3:
            score += max_score * 0.2
        elif list_items >= 1:
            score += max_score * 0.1
        else:
            suggestions.append({
                "text": "å»ºè­°ä½¿ç”¨é …ç›®ç¬¦è™Ÿæˆ– emoji æ•´ç†é‡é»ï¼Œæå‡å¯è®€æ€§",
                "priority": 4,
            })

        # åˆ†éš”ç·šä½¿ç”¨ï¼ˆä½” 20%ï¼‰
        separator_count = len(re.findall(r'={3,}|â€”{3,}|â”€{3,}', content))
        if separator_count >= 2:
            score += max_score * 0.2
        elif separator_count >= 1:
            score += max_score * 0.1
        else:
            score += max_score * 0.05

        return round(score, 1)

    def _score_content_length(self, content: str, suggestions: list) -> float:
        """å…§å®¹é•·åº¦è©•åˆ†ï¼š1500-2500 å­—æ»¿åˆ†"""
        max_score = self.WEIGHTS["content_length"]
        content_len = len(content)

        if 1500 <= content_len <= 2500:
            return max_score
        elif 1000 <= content_len < 1500:
            suggestions.append({
                "text": f"å…§å®¹ {content_len} å­—ï¼Œå»ºè­°æ“´å……åˆ° 1500-2500 å­—ä»¥ç²å¾—æœ€ä½³ SEO æ•ˆæœ",
                "priority": 2,
            })
            return round(max_score * 0.7, 1)
        elif 2500 < content_len <= 3500:
            return round(max_score * 0.85, 1)
        elif content_len > 3500:
            return round(max_score * 0.7, 1)
        elif 500 <= content_len < 1000:
            suggestions.append({
                "text": f"å…§å®¹åƒ… {content_len} å­—ï¼ŒSEO æ•ˆæœæœ‰é™ï¼Œå»ºè­°è‡³å°‘ 1500 å­—",
                "priority": 1,
            })
            return round(max_score * 0.4, 1)
        else:
            suggestions.append({
                "text": f"å…§å®¹åƒ… {content_len} å­—ï¼Œé ä½æ–¼ SEO å»ºè­°çš„ 1500 å­—",
                "priority": 1,
            })
            return round(max_score * 0.15, 1)

    def _score_faq_quality(self, content: str, suggestions: list) -> float:
        """FAQ çµæ§‹è©•åˆ†ï¼šåµæ¸¬ Q/A æ ¼å¼ã€å•ç­”é¡Œæ•¸"""
        max_score = self.WEIGHTS["faq_quality"]

        # åµæ¸¬ FAQ æ¨¡å¼
        # Q1: / A1: æ ¼å¼
        qa_pattern = re.findall(r'Q\d+[:ï¼š]', content, re.IGNORECASE)
        # â“ é–‹é ­çš„å•é¡Œ
        emoji_q = re.findall(r'â“\s*.+', content)
        # ã€Œå¸¸è¦‹å•é¡Œã€ã€ŒFAQã€å€å¡Š
        has_faq_section = bool(re.search(r'(FAQ|å¸¸è¦‹å•é¡Œ|Q&A|å•ç­”)', content, re.IGNORECASE))
        # å•è™Ÿçµå°¾çš„è¡Œï¼ˆå¯èƒ½æ˜¯å•é¡Œï¼‰
        question_lines = re.findall(r'^.{5,}[ï¼Ÿ?]\s*$', content, re.MULTILINE)

        faq_count = max(len(qa_pattern) // 2, len(emoji_q), len(question_lines))

        if faq_count >= 3 and has_faq_section:
            return max_score
        elif faq_count >= 3:
            return round(max_score * 0.8, 1)
        elif faq_count >= 1:
            suggestions.append({
                "text": f"åƒ…åµæ¸¬åˆ° {faq_count} å€‹ FAQï¼Œå»ºè­°è‡³å°‘ 3 å€‹ä»¥æå‡ Google AI Overview å¼•ç”¨ç‡",
                "priority": 3,
            })
            return round(max_score * 0.4, 1)
        else:
            suggestions.append({
                "text": "ç¼ºå°‘ FAQ å€å¡Šï¼ŒFAQ çµæ§‹èƒ½è®“ Google AI Overview å¼•ç”¨ç‡æå‡ 3.2 å€",
                "priority": 2,
            })
            return 0.0

    def _score_media_usage(self, content: str, suggestions: list) -> tuple:
        """åœ–ç‰‡ä½¿ç”¨è©•åˆ†"""
        max_score = self.WEIGHTS["media_usage"]
        image_count = len(re.findall(r'\{\{IMAGE:\d+:\d+\}\}', content))
        # ä¹Ÿè¨ˆç®—å·²æ›¿æ›çš„ markdown åœ–ç‰‡
        image_count += len(re.findall(r'!\[.*?\]\(.*?\)', content))

        if image_count >= 3:
            return max_score, image_count
        elif image_count >= 1:
            suggestions.append({
                "text": f"ç›®å‰æœ‰ {image_count} å¼µåœ–ç‰‡ï¼Œå»ºè­°è‡³å°‘ 3 å¼µè±å¯Œè¦–è¦ºå…§å®¹",
                "priority": 4,
            })
            return round(max_score * 0.6, 1), image_count
        else:
            suggestions.append({
                "text": "æ²’æœ‰åœ–ç‰‡ï¼Œå»ºè­°åŠ å…¥å•†å“åœ–ç‰‡æå‡æ–‡ç« å¸å¼•åŠ›",
                "priority": 3,
            })
            return 0.0, image_count

    def _score_readability(self, content: str, suggestions: list) -> float:
        """å¯è®€æ€§è©•åˆ†ï¼šçœŸå¯¦åˆ†æå¥é•· + emoji ä½¿ç”¨"""
        max_score = self.WEIGHTS["readability"]
        score = 0.0

        # å¥é•·åˆ†æï¼ˆä½” 60%ï¼‰ï¼šä»¥å¥è™Ÿ/å•è™Ÿ/é©šå˜†è™Ÿç‚ºæ–·å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n]+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) >= 5]
        if sentences:
            avg_sentence_len = sum(len(s) for s in sentences) / len(sentences)
            if 15 <= avg_sentence_len <= 50:
                score += max_score * 0.6
            elif 10 <= avg_sentence_len <= 70:
                score += max_score * 0.4
            else:
                score += max_score * 0.15
                suggestions.append({
                    "text": f"å¹³å‡å¥é•· {avg_sentence_len:.0f} å­—ï¼Œå»ºè­°æ§åˆ¶åœ¨ 15-50 å­—",
                    "priority": 4,
                })
        else:
            score += max_score * 0.2

        # Emoji ä½¿ç”¨ï¼ˆä½” 40%ï¼‰ï¼šDcard é¢¨æ ¼é¼“å‹µé©åº¦ä½¿ç”¨ emoji
        emoji_pattern = re.compile(
            r'[\U0001F300-\U0001F9FF\U00002600-\U000027BF\U0000FE00-\U0000FEFF]'
        )
        emoji_count = len(emoji_pattern.findall(content))
        if 3 <= emoji_count <= 30:
            score += max_score * 0.4
        elif emoji_count > 0:
            score += max_score * 0.25
        else:
            score += max_score * 0.1

        return round(score, 1)

    @staticmethod
    def _get_grade(score: float) -> str:
        if score >= 85:
            return "A"
        elif score >= 70:
            return "B"
        elif score >= 50:
            return "C"
        else:
            return "D"

    def optimize_with_llm(self, article, model: Optional[str] = None, user_id: Optional[int] = None) -> dict:
        """ä½¿ç”¨ LLM é€²è¡Œ SEO å„ªåŒ–"""
        content = article.content or ""

        # å…ˆåˆ†æç¾ç‹€
        before_analysis = self.analyze(title=article.title, content=content)
        keywords = before_analysis.get("keywords", [])

        # çµ„åˆåŒ…å«å…·é«”é—œéµå­—å’Œåˆ†æ•¸æ˜ç´°çš„è¨Šæ¯
        breakdown_text = ""
        for key, item in before_analysis["breakdown"].items():
            pct = round(item["score"] / item["max"] * 100) if item["max"] > 0 else 0
            breakdown_text += f"  - {item['label']}: {item['score']}/{item['max']} ({pct}%)\n"

        user_message = f"""ä»¥ä¸‹æ˜¯éœ€è¦ SEO å„ªåŒ–çš„æ–‡ç« ï¼š

æ¨™é¡Œï¼š{article.title}

å…§å®¹ï¼š
{content}

ç›®æ¨™çœ‹æ¿ï¼š{article.target_forum}

=== SEO åˆ†æçµæœ ===
ç¸½åˆ†ï¼š{before_analysis['score']}/{before_analysis['max_score']} ({before_analysis['grade']})
åˆ†é …ï¼š
{breakdown_text}
æå–çš„é—œéµå­—ï¼š{', '.join(keywords)}

ç¾æœ‰ SEO å•é¡Œï¼š
{chr(10).join(f'- {s}' for s in before_analysis['suggestions'])}

è«‹ç‰¹åˆ¥æ³¨æ„ï¼š
- ä¸»è¦é—œéµå­—ï¼š{', '.join(keywords[:3])}
- é¦–æ®µ 100 å­—å…§è¦å¸¶å…¥ä¸»è¦é—œéµå­—
- é—œéµå­—å¯†åº¦ç›®æ¨™ 1-2%
- FAQ ç”¨ Q1:/A1: çµæ§‹åŒ–æ ¼å¼
"""

        # SEO å„ªåŒ–å¼·åˆ¶ä½¿ç”¨æœ€ä¾¿å®œçš„æ¨¡å‹ï¼ˆç¯€çœæˆæœ¬ï¼ŒSEO æ”¹å¯«ä¸éœ€è¦é«˜éšæ¨¡å‹ï¼‰
        use_model = "gemini-2.5-flash"
        try:
            if is_anthropic_model(use_model):
                response = self.anthropic_client.messages.create(
                    model=use_model,
                    max_tokens=settings.LLM_MAX_TOKENS,
                    system=SEO_OPTIMIZE_PROMPT,
                    messages=[{"role": "user", "content": user_message}],
                )
                optimized_content = response.content[0].text
                logger.info(f"Claude SEO å„ªåŒ–å®Œæˆï¼Œæ–‡å­—é•·åº¦: {len(optimized_content)}")
                track_anthropic_usage(response, model=use_model, user_id=user_id)
            else:
                response = self.gemini_client.models.generate_content(
                    model=use_model,
                    contents=user_message,
                    config=types.GenerateContentConfig(
                        system_instruction=SEO_OPTIMIZE_PROMPT,
                        temperature=0.5,
                        max_output_tokens=settings.LLM_MAX_TOKENS,
                        http_options=types.HttpOptions(timeout=300_000),  # æ¯«ç§’ï¼Œ300ç§’
                    ),
                )
                optimized_content = response.text
                logger.info(f"Gemini SEO å„ªåŒ–å®Œæˆï¼Œæ–‡å­—é•·åº¦: {len(optimized_content)}")
                track_gemini_usage(response, model=use_model, user_id=user_id)

            # æ¸…é™¤å¯èƒ½æ®˜ç•™çš„ Markdown
            optimized_content = strip_markdown(optimized_content)

            # å¾ LLM è¼¸å‡ºä¸­è§£æå„ªåŒ–å¾Œçš„æ¨™é¡Œï¼ˆç¬¬ä¸€å€‹éç©ºè¡Œï¼‰
            optimized_title = article.title  # é è¨­ä¿ç•™åŸæ¨™é¡Œ
            lines = optimized_content.strip().split('\n')
            skip_patterns = {'---', '===', '***', '- - -', '* * *'}
            for i, line in enumerate(lines):
                stripped = line.strip()
                if not stripped or stripped in skip_patterns:
                    continue
                # ç¬¬ä¸€å€‹æœ‰æ„ç¾©çš„è¡Œä½œç‚ºæ¨™é¡Œ
                optimized_title = stripped.lstrip('#').strip()
                optimized_content = '\n'.join(lines[i + 1:]).strip()
                break

            # å„ªåŒ–å¾Œé‡æ–°åˆ†æï¼ˆä½¿ç”¨æ–°æ¨™é¡Œï¼‰
            after_analysis = self.analyze(title=optimized_title, content=optimized_content)

            return {
                "optimized_title": optimized_title,
                "optimized_content": optimized_content,
                "score": after_analysis["score"],
                "suggestions": after_analysis["suggestions"],
                "before_score": before_analysis["score"],
                "before_analysis": before_analysis,
                "after_analysis": after_analysis,
            }

        except Exception as e:
            logger.error(f"SEO LLM å„ªåŒ–å¤±æ•— ({use_model}): {e}")
            raise RuntimeError(f"SEO å„ªåŒ–å¤±æ•—: {e}")


# å–®ä¾‹
seo_service = SeoService()
